{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune BERT LLM for Label Prediction with Kubeflow PyTorchJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook requires:\n",
    "\n",
    "- A working Kubeflow installation. \n",
    "- At least **2 GPU** on your Kubernetes cluster to fine-tune BERT model on 2 workers.\n",
    "- GCS bucket to download the custom dataset and export the fine-tuned model.\n",
    "\n",
    "You might also want to make yourself familiar with the \"local\" version of the finetuning, shown in `fine_tuning_local.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets gcsfs\n",
    "!pip install git+https://github.com/kubeflow/training-operator.git#subdirectory=sdk/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create script to fine-tune BERT model\n",
    "\n",
    "We need to wrap our fine-tuning logic in a function to create Kubeflow PyTorchJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(parameters):\n",
    "    import os\n",
    "    import gcsfs\n",
    "    import evaluate\n",
    "    import numpy as np\n",
    "    from datasets import load_dataset\n",
    "    from datasets.distributed import split_dataset_by_node\n",
    "    from transformers import (\n",
    "        AutoModelForSequenceClassification,\n",
    "        AutoTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "\n",
    "    model_name = parameters['MODEL_NAME']\n",
    "    storage_options= parameters['STORAGE_OPTIONS'] \n",
    "    dataset = load_dataset(\"json\", data_files=f'gs://{parameters[\"BUCKET\"]}/{parameters[\"DATASET_FILE\"]}', storage_options=storage_options)\n",
    "    ds = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "    \n",
    "    labels = [label for label in ds['train'].features.keys() if label not in ['body', 'title']]\n",
    "    id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "    label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Download BERT Model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "            \n",
    "    def preprocess_data(example):\n",
    "      text = f'{example[\"title\"]}\\n{example[\"body\"]}'\n",
    "      # encode them\n",
    "      encoding = tokenizer(text, padding=True, truncation=True)\n",
    "    \n",
    "      lbls = [0. for i in range(len(labels))]\n",
    "      for label in labels:\n",
    "        if label in example and example[label] == True:\n",
    "          label_id = label2id[label]\n",
    "          lbls[label_id] = 1.\n",
    "    \n",
    "      encoding[\"labels\"] = lbls  \n",
    "      return encoding\n",
    "    \n",
    "    # Map custom dataset to BERT tokenizer.\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Map dataset to BERT Tokenizer\")\n",
    "    encoded_dataset = ds.map(preprocess_data, remove_columns=ds['train'].column_names)\n",
    "    encoded_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # Distribute train and test datasets between PyTorch workers.\n",
    "    # Every worker will process chunk of training data.\n",
    "    # RANK and WORLD_SIZE will be set by Kubeflow Training Operator.\n",
    "    RANK = int(os.environ[\"RANK\"])\n",
    "    WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"])\n",
    "    distributed_ds_train = split_dataset_by_node(\n",
    "        encoded_dataset[\"train\"],\n",
    "        rank=RANK,\n",
    "        world_size=WORLD_SIZE,\n",
    "    )\n",
    "    distributed_ds_test = split_dataset_by_node(\n",
    "        encoded_dataset[\"test\"],\n",
    "        rank=RANK,\n",
    "        world_size=WORLD_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Evaluate accuracy.    \n",
    "    clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "    def sigmoid(x):\n",
    "       return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "       predictions, labels = eval_pred\n",
    "       predictions = sigmoid(predictions)\n",
    "       predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "       return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))\n",
    "\n",
    "\n",
    "    batch_size = 3\n",
    "    metric_name = \"f1\"\n",
    "    args = TrainingArguments(\n",
    "        f\"{model_name}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=metric_name,\n",
    "    )\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=distributed_ds_train,\n",
    "        eval_dataset=distributed_ds_test,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Start Distributed Training. RANK: {RANK} WORLD_SIZE: {WORLD_SIZE}\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Training is complete\")\n",
    "    \n",
    "    # Export trained model to GCS from the worker with RANK = 0 (master).\n",
    "    if RANK == 0:\n",
    "        trainer.save_model(f\"./{model_name}\")\n",
    "        fs = gcsfs.GCSFileSystem(**storage_options)\n",
    "        files = ['config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer_config.json', 'tokenizer.json', 'training_args.bin', 'vocab.txt']\n",
    "        for f in files: \n",
    "            fs.put(f'{model_name}/{f}', f'{parameters[\"BUCKET\"]}/{model_name}/{f}')\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Model export complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Kubeflow PyTorchJob to fine-tune BERT on GPUs\n",
    "\n",
    "Use `TrainingClient()` to create PyTorchJob which will fine-tune BERT on **2 workers** using **1 GPU** for each worker.\n",
    "\n",
    "Your Kubernetes cluster should have sufficient **GPU** resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "\n",
    "job_name = \"fine-tune-bert-label-prediction\"\n",
    "bucket = \"label-prediction\"\n",
    "model_name = \"bert-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create PyTorchJob\n",
    "TrainingClient().create_job(\n",
    "    name=job_name,\n",
    "    train_func=train_func,\n",
    "    parameters={\n",
    "        \"BUCKET\": bucket,\n",
    "        \"STORAGE_OPTIONS\": {\"project\": \"<REPLACE-WITH-YOUR-GCLOUD-PROJECT-ID>\", \"token\": \"google_default\"},\n",
    "        \"MODEL_NAME\": model_name,\n",
    "        \"DATASET_FILE\": \"prepared-issues-reduced.json\"\n",
    "    },\n",
    "    num_workers=2,  # Number of PyTorch workers to use.\n",
    "    resources_per_worker={\n",
    "        \"cpu\": \"3\",\n",
    "        \"memory\": \"10G\",\n",
    "        \"gpu\": \"1\",\n",
    "    },\n",
    "    packages_to_install=[\n",
    "        \"gcsfs\",\n",
    "        \"transformers\",\n",
    "        \"datasets==2.16\",\n",
    "        \"evaluate\",\n",
    "        \"accelerate\",\n",
    "        \"scikit-learn\",\n",
    "    ],  # PIP packages will be installed during PyTorchJob runtime.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check the PyTorchJob conditions\n",
    "\n",
    "Use `TrainingClient()` APIs to get information about created PyTorchJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchJob Conditions\n",
      "[{'last_transition_time': datetime.datetime(2024, 10, 28, 10, 46, tzinfo=tzlocal()),\n",
      " 'last_update_time': datetime.datetime(2024, 10, 28, 10, 46, tzinfo=tzlocal()),\n",
      " 'message': 'PyTorchJob fine-tune-bert-label-prediction is created.',\n",
      " 'reason': 'PyTorchJobCreated',\n",
      " 'status': 'True',\n",
      " 'type': 'Created'}]\n",
      "----------------------------------------\n",
      "PyTorchJob is running\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorchJob Conditions\")\n",
    "print(TrainingClient().get_job_conditions(job_name))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Wait until PyTorchJob has Running condition.\n",
    "job = TrainingClient().wait_for_job_conditions(\n",
    "    job_name,\n",
    "    expected_conditions={\"Running\"},\n",
    ")\n",
    "print(\"PyTorchJob is running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the PyTorchJob pod names\n",
    "\n",
    "Since we set 2 workers, PyTorchJob will create 1 master pod and 1 worker pod to execute distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine-tune-bert-label-prediction-master-0',\n",
       " 'fine-tune-bert-label-prediction-worker-0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingClient().get_job_pod_names(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-01T20:10:25.759950Z",
     "iopub.status.idle": "2022-09-01T20:10:25.760581Z",
     "shell.execute_reply": "2022-09-01T20:10:25.760353Z",
     "shell.execute_reply.started": "2022-09-01T20:10:25.760328Z"
    },
    "tags": []
   },
   "source": [
    "### Get the PyTorchJob training logs\n",
    "\n",
    "Every worker processes a part of the training samples on each epoch since we distribute trianing across 2 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pod fine-tune-bert-label-prediction-master-0]: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276k/276k [00:00<00:00, 84.3MB/s]\n",
      "Generating train split: 359 examples [00:00, 47251.24 examples/s]\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: ----------------------------------------\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: Download BERT Model\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: ----------------------------------------\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: Map dataset to BERT Tokenizer\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287/287 [00:00<00:00, 1331.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:00<00:00, 1108.36 examples/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.20k/4.20k [00:00<00:00, 12.1MB/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.77k/6.77k [00:00<00:00, 22.4MB/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.55k/7.55k [00:00<00:00, 23.7MB/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.36k/7.36k [00:00<00:00, 23.7MB/s]\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: /opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "[Pod fine-tune-bert-label-prediction-master-0]:   warnings.warn(\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: /tmp/tmp.cTabkj7WG0/ephemeral_script.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "[Pod fine-tune-bert-label-prediction-master-0]:   trainer = Trainer(\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: ----------------------------------------\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: Start Distributed Training. RANK: 0 WORLD_SIZE: 2\n",
      "  0%|          | 0/120 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 20%|â–ˆâ–ˆ        | 24/120 [00:17<00:54,  1.77it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[Ater-0]: \n",
      "                                                \n",
      "[Pod fine-tune-bert-label-prediction-master-0]: {'eval_loss': 0.5807374119758606, 'eval_accuracy': 0.6805555555555556, 'eval_f1': 0.6229508196721312, 'eval_precision': 0.5277777777777778, 'eval_recall': 0.76, 'eval_runtime': 0.2459, 'eval_samples_per_second': 146.416, 'eval_steps_per_second': 24.403, 'epoch': 1.0}\n",
      " 20%|â–ˆâ–ˆ        | 24/120 [00:18<00:54,  1.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 39.85it/s]\u001B[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:40<01:22,  1.15s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[Ater-0]: \n",
      "                                                \n",
      "[Pod fine-tune-bert-label-prediction-master-0]: {'eval_loss': 0.5413233041763306, 'eval_accuracy': 0.7222222222222222, 'eval_f1': 0.6551724137931034, 'eval_precision': 0.5757575757575758, 'eval_recall': 0.76, 'eval_runtime': 0.2467, 'eval_samples_per_second': 145.928, 'eval_steps_per_second': 24.321, 'epoch': 2.0}\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 48/120 [00:40<01:22,  1.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 39.71it/s]\u001B[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [01:02<00:48,  1.02s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[Ater-0]: \n",
      "                                                \n",
      "[Pod fine-tune-bert-label-prediction-master-0]: {'eval_loss': 0.49169281125068665, 'eval_accuracy': 0.7222222222222222, 'eval_f1': 0.6551724137931034, 'eval_precision': 0.5757575757575758, 'eval_recall': 0.76, 'eval_runtime': 0.2472, 'eval_samples_per_second': 145.648, 'eval_steps_per_second': 24.275, 'epoch': 3.0}\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 72/120 [01:03<00:48,  1.02s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 40.03it/s]\u001B[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [01:22<00:18,  1.32it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[Ater-0]: \n",
      "                                                \n",
      "[Pod fine-tune-bert-label-prediction-master-0]: {'eval_loss': 0.4143230617046356, 'eval_accuracy': 0.8055555555555556, 'eval_f1': 0.7307692307692307, 'eval_precision': 0.7037037037037037, 'eval_recall': 0.76, 'eval_runtime': 0.2447, 'eval_samples_per_second': 147.145, 'eval_steps_per_second': 24.524, 'epoch': 4.0}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [01:22<00:18,  1.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 40.44it/s]\u001B[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [01:40<00:00,  1.71it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[Ater-0]: \n",
      "[Pod fine-tune-bert-label-prediction-master-0]: {'eval_loss': 0.39437583088874817, 'eval_accuracy': 0.7777777777777778, 'eval_f1': 0.7037037037037037, 'eval_precision': 0.6551724137931034, 'eval_recall': 0.76, 'eval_runtime': 0.2161, 'eval_samples_per_second': 166.563, 'eval_steps_per_second': 27.76, 'epoch': 5.0}\n",
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [01:42<00:00,  1.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 37.47it/s]\u001B[A\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: {'train_runtime': 108.9736, 'train_samples_per_second': 6.607, 'train_steps_per_second': 1.101, 'train_loss': 0.4860968589782715, 'epoch': 5.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [01:47<00:00,  1.11it/s]\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: ----------------------------------------\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: Training is complete\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: ----------------------------------------\n",
      "[Pod fine-tune-bert-label-prediction-master-0]: Model export complete\n"
     ]
    }
   ],
   "source": [
    "logs, _ = TrainingClient().get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you see \"Model export complete\" in the logs the training is done! \n",
    "\n",
    "If you are trying to run it again, make sure to delete the Job resource or rename the Job to avoid name conflicts. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the fine-tuned model\n",
    "\n",
    "We can download our fine-tuned BERT model from GCS to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import os\n",
    "\n",
    "storage_options = {\"project\": \"<REPLACE-WITH-YOUR-GCLOUD-PROJECT-ID>\", \"token\": \"google_default\"},\n",
    "fs = gcsfs.GCSFileSystem(**storage_options)\n",
    "\n",
    "# config.json is the model metadata.\n",
    "# model.safetensors is the model weights & biases.\n",
    "local_name = \"bert-local\"\n",
    "if not os.path.exists(local_name):\n",
    "    os.makedirs(local_name)\n",
    "files = ['config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer_config.json', 'tokenizer.json', 'training_args.bin', 'vocab.txt']\n",
    "for f in files:\n",
    "    fs.get(f'{bucket}/{model_name}/{f}', f'{local_name}/{f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the fine-tuned BERT model\n",
    "\n",
    "We are going to use HuggingFace pipeline to test our model.\n",
    "\n",
    "We will ask for sentiment analysis task for our fine-tuned LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there should be a new button on the package detail page right below the header\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# During fine-tuning BERT tokenizer is not changed.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Use pipeline with sentiment-analysis task to evaluate our model.\n",
    "nlp = pipeline(\"sentiment-analysis\", model=\"./bert-local\", tokenizer=tokenizer)\n",
    "\n",
    "bug = \"\"\"\n",
    "**Describe the bug**\n",
    "glasskube installation doesn't work. \n",
    "\n",
    "**To reproduce**\n",
    "Run `glasskube bootstrap`. \n",
    "\n",
    "**Cluster Info (please complete the following information):**\n",
    "\"\"\"\n",
    "\n",
    "enhancement = \"\"\"\n",
    "**Is your feature request related to a problem? Please describe.**\n",
    "There are not enough cool buttons on the UI, please add more buttons I can click, that gives me more feelings of power!\n",
    "\n",
    "**Describe the solution you'd like**\n",
    "Create a button on the package detail page that says \"Check for Updates\" or something like that. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(nlp(bug))\n",
    "print(nlp(enhancement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T23:44:15.511173Z",
     "iopub.status.busy": "2024-03-01T23:44:15.510932Z",
     "iopub.status.idle": "2024-03-01T23:44:15.539921Z",
     "shell.execute_reply": "2024-03-01T23:44:15.539352Z",
     "shell.execute_reply.started": "2024-03-01T23:44:15.511155Z"
    },
    "tags": []
   },
   "source": [
    "## Delete the PyTorchJobs\n",
    "\n",
    "When done with the training, you can delete the created PyTorchJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainingClient().delete_job(name=job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
